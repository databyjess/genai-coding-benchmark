# GenAI Coding Benchmark

A **practical, reproducible benchmark** for evaluating browser-based generative AI models on real-world **Python, SQL, and DAX** coding tasks.

This repository focuses on **code correctness, efficiency, and maintainability** under controlled conditions, with the goal of supporting **evidence-based decisions** about GenAI adoption in data engineering, analytics, and BI teams.

---

## Motivation

Generative AI tools are increasingly used to write production code, yet most comparisons are anecdotal, tool-driven, or lack methodological rigour.

This project addresses that gap by providing:

* A **controlled experimental setup**
* **Reproducible prompts and datasets**
* **Automated testing** where possible
* **Structured human evaluation** where automation is not feasible (DAX)

The emphasis is on **practical enterprise tasks**, not toy examples.

---

## Research question

**Which browser-based generative AI model currently produces the most correct, efficient, and maintainable solutions for data-related coding tasks in Python, SQL, and DAX under controlled conditions?**

Secondary questions include:

* How consistent are models across different programming paradigms?
* Where do models systematically fail (logic, syntax, performance, context handling)?
* Are observed differences practically meaningful for enterprise use?

---

## Scope

### In scope

* Browser-based GenAI systems (e.g. ChatGPT, Claude, Gemini, Microsoft Copilot)
* Python 3.11 (data processing, algorithms, validation)
* SQL (ANSI / Postgres-compatible analytical queries)
* DAX (measures, filter context, time intelligence)
* Unit-test driven evaluation where feasible

### Out of scope

* IDE integrations (GitHub Copilot, Gemini Code Assist, etc.)
* Inline code completion or UX evaluation
* Agentic workflows and autonomous coding agents
* Non-data-related programming tasks

---

## Repository structure

```
â”œâ”€â”€ tasks/          # Machine-readable task definitions (JSON)
â”œâ”€â”€ datasets/       # Input datasets for tasks
â”œâ”€â”€ prompts/        # Prompt templates per language
â”œâ”€â”€ outputs/        # Raw model outputs (by task / model / run)
â”œâ”€â”€ tests/          # Automated tests (Python / SQL)
â”œâ”€â”€ scoring/        # Scoring logic and DAX review templates
â”œâ”€â”€ results/        # Aggregated benchmark results
â”œâ”€â”€ methodology.md  # Detailed experimental methodology
â””â”€â”€ README.md
```

---

## Evaluation approach

### Python & SQL

* Automated unit tests (functional correctness)
* Performance and basic memory checks (task-dependent)
* Static analysis for code quality and common error patterns
* Latency recorded per model response

### DAX

* Structured expert review using predefined checklists
* Focus on:

  * Correct context handling
  * Proper use of CALCULATE / FILTER
  * Time intelligence correctness
  * Safe division and BLANK handling

DAX results are reported **separately** to avoid skewing overall rankings.

---

## Scoring (high level)

Scores are computed per task and aggregated per language.

Indicative weighting:

* Functional correctness: 40%
* Performance & efficiency: 15%
* Code quality & safety: 20%
* Robustness & explanation quality: 15%
* Latency (proxy for usability): 10%

Exact scoring logic is documented in `scoring/`.

---

## Reproducibility

* All prompts are version-controlled
* Datasets are fixed and included
* Model versions and execution dates are logged
* Results can be re-generated by re-running the evaluation pipeline

This benchmark is **time-bound**: results reflect model behaviour at the time of execution.

---

## What this benchmark does *not* measure

* Developer experience or IDE ergonomics
* Inline completion quality
* Multi-file refactoring in large codebases
* Long-running agent behaviour

These aspects are intentionally excluded to preserve experimental control.

---

## Licence

This project is licensed under the **Apache License 2.0**.

You are free to use, modify, and distribute this code in compliance with the licence terms.

---

## Disclaimer

This repository is provided for **research and informational purposes only**. Benchmark results should not be interpreted as absolute or permanent rankings, as GenAI models evolve rapidly.

---

## Contributing

Contributions are welcome.

Typical contributions include:

* New benchmark tasks
* Improved tests or scoring logic
* Additional result analysis

Please ensure any contribution maintains methodological consistency and reproducibility.

---

## Citation

If you reference this benchmark internally or externally, please cite the repository and the execution date of the results.

---

## Status

ðŸš§ Initial public release â€” benchmark tasks and results will evolve over time.
